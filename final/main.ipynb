{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99006d5a",
   "metadata": {},
   "source": [
    "# Classification Task â€“ Dataset Loading & Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe4b36",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d435c96",
   "metadata": {},
   "source": [
    "# 1. Notebook Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ff9a3f",
   "metadata": {},
   "source": [
    "## 1.1. Install necessary libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8843293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Upgrade pip and install Jupyter Notebook and other dependencies\n",
    "%pip install --upgrade notebook ipywidgets nbformat nbimporter import_ipynb --quiet\n",
    "\n",
    "# Install ucimlrepo package for accessing UCI ML datasets\n",
    "%pip install ucimlrepo --quiet\n",
    "\n",
    "# Install kaeggle package for Kaggle datasets\n",
    "%pip install kaggle --quiet\n",
    "\n",
    "%pip install pandas numpy scikit-learn plotly seaborn --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a5145",
   "metadata": {},
   "source": [
    "## 1.2 Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c044ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2368838f",
   "metadata": {},
   "source": [
    "## 1.3. Create Utils & Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a8d80d",
   "metadata": {},
   "source": [
    "### 1.3.1. Logger Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a26673b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "        Initializes a logger for the class with the given name.\n",
    "        \n",
    "        Args:\n",
    "            name (str): The name to be used for the logger. It is typically \n",
    "                        the name of the module or class.\n",
    "        \n",
    "        This method configures the logger to write log messages to the console\n",
    "        using a StreamHandler and sets the log level to DEBUG. It ensures that \n",
    "        handlers are added only once to prevent duplicate log entries.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create a logger with the provided name\n",
    "        self.logger = logging.getLogger(name)\n",
    "        \n",
    "        # Check if the logger already has any handlers to avoid duplicate logs\n",
    "        if not self.logger.hasHandlers():\n",
    "            # Create a stream handler to log messages to the console\n",
    "            handler = logging.StreamHandler()\n",
    "            \n",
    "            # Set up a log message format: timestamp, logger name, log level, and message\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            handler.setFormatter(formatter)  # Apply the formatter to the handler\n",
    "            \n",
    "            # Add the handler to the logger\n",
    "            self.logger.addHandler(handler)\n",
    "            \n",
    "            # Set the log level to DEBUG, so all messages of level DEBUG and above are shown\n",
    "            self.logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        \"\"\"\n",
    "        Delegates attribute access to the internal logger instance.\n",
    "        \n",
    "        This method is invoked when an attribute that doesn't exist in the \n",
    "        current object is accessed. It forwards the request to the logger \n",
    "        instance, allowing access to all logging-related attributes and methods \n",
    "        as if they were part of the object.\n",
    "\n",
    "        Args:\n",
    "            attr (str): The name of the attribute being accessed.\n",
    "\n",
    "        Returns:\n",
    "            The value of the attribute from the internal logger instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # Forward the attribute access to the logger object\n",
    "        return getattr(self.logger, attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "898f4f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logger instance for the current module\n",
    "logger = Logger(__name__)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac14ea9",
   "metadata": {},
   "source": [
    "### 1.3.2. Data Set Analysis Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67782e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_dimensions(df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"\n",
    "    Returns the dimensions of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        - df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        - tuple: A tuple containing the number of rows and columns in the DataFrame.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Dataset dimensions: {df.shape}\")\n",
    "    return df.shape  # Returns a tuple (number of rows, number of columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a12915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_values(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with the counts of unique values for specified columns.\n",
    "\n",
    "    Parameters:\n",
    "        - df (pd.DataFrame): The input DataFrame.\n",
    "        - columns (list): List of column names to count unique values for.\n",
    "\n",
    "    Returns:\n",
    "        - pd.DataFrame: A DataFrame with columns ['feature', unique values...]\n",
    "        where each row corresponds to a feature and the count of each unique value.\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to collect counts\n",
    "    value_counts = {}\n",
    "    \n",
    "    # Iterate over each specified column\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            counts = df[col].value_counts(dropna=False)\n",
    "            value_counts[col] = counts\n",
    "        else:\n",
    "            raise KeyError(f\"Column '{col}' does not exist in the DataFrame.\")\n",
    "    \n",
    "    # Create a combined DataFrame, filling missing values with 0\n",
    "    counts_df = pd.DataFrame(value_counts).fillna(0).astype(int).T\n",
    "    counts_df.index.name = 'feature'\n",
    "    \n",
    "    return counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b8cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_missing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarizes the count and percentage of missing values in each column of the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        - df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        - pd.DataFrame: A DataFrame with two columns:\n",
    "        - 'missing_count': The count of missing values per column.\n",
    "        - 'missing_pct': The percentage of missing values per column.\n",
    "    \"\"\"\n",
    "    total = len(df)  # Get the total number of rows in the dataframe\n",
    "    missing_count = df.isna().sum()  # Count missing values per column\n",
    "    missing_pct = missing_count / total * 100  # Calculate the percentage of missing values\n",
    "    return pd.DataFrame({\n",
    "        \"missing_count\": missing_count,\n",
    "        \"missing_pct\": missing_pct\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bca327",
   "metadata": {},
   "source": [
    "### 1.3.3. Data Processing Missing Values Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f0491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_values(df: pd.DataFrame, columns: list, replace_map: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replaces specified values in given columns according to a replacement map.\n",
    "\n",
    "    Parameters:\n",
    "        - df (pd.DataFrame): The input DataFrame.\n",
    "        - columns (list): List of column names where replacements should occur.\n",
    "        - replace_map (dict): Dictionary specifying which values to replace with what.\n",
    "        Example: {'unknown': np.nan, 'yes': 1, 'no': 0}\n",
    "\n",
    "    Returns:\n",
    "        - pd.DataFrame: A new DataFrame with the values replaced.\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid modifying the original DataFrame\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    \n",
    "    # Iterate over each specified column and apply the replacement\n",
    "    for col in columns:\n",
    "        if col in df_copy.columns:\n",
    "            # Replace values\n",
    "            df_copy[col] = df_copy[col].replace(replace_map)\n",
    "\n",
    "            # Explicitly fix object types\n",
    "            df_copy[col] = df_copy[col].infer_objects(copy=False)\n",
    "        else:\n",
    "            raise KeyError(f\"Column '{col}' does not exist in the DataFrame.\")\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8679f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_missing(df: pd.DataFrame, threshold: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drops columns with a missing value percentage greater than the specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "        - df (pd.DataFrame): The input DataFrame.\n",
    "        - threshold (float): The percentage threshold for missing values (0.0 - 1.0).\n",
    "        Columns with more missing values than this threshold will be dropped.\n",
    "\n",
    "    Returns:\n",
    "        - pd.DataFrame: A copy of the DataFrame with columns dropped where missing values exceed the threshold.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()  # Work with a copy to avoid modifying the original DataFrame\n",
    "    miss = df_copy.isna().mean()  # Calculate the percentage of missing values per column\n",
    "    cols_to_drop = miss[miss > threshold].index  # Find columns with more missing values than the threshold\n",
    "    return df_copy.drop(columns=cols_to_drop)  # Drop those columns and return the cleaned DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a429803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows_missing(df: pd.DataFrame, threshold: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drops rows with a missing value percentage greater than the specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "        - df (pd.DataFrame): The input DataFrame.\n",
    "        - threshold (float): The percentage threshold for missing values (0.0 - 1.0).\n",
    "        Rows with more missing values than this threshold will be dropped.\n",
    "\n",
    "    Returns:\n",
    "        - pd.DataFrame: A copy of the DataFrame with rows dropped where missing values exceed the threshold.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()  # Work with a copy to avoid modifying the original DataFrame\n",
    "    mask = df_copy.isna().mean(axis=1) <= threshold  # Create a mask to keep rows with less missing data\n",
    "    return df_copy.loc[mask].copy()  # Apply the mask and return the cleaned DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_mode(df: pd.DataFrame, columns: list = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Imputes missing values in the specified columns using the mode (most frequent value).\n",
    "\n",
    "    Parameters:\n",
    "        - df (pd.DataFrame): The input DataFrame.\n",
    "        - columns (list, optional): A list of column names where missing values should be imputed.\n",
    "        If None, all columns with missing values will be considered.\n",
    "\n",
    "    Returns:\n",
    "        - pd.DataFrame: A copy of the DataFrame with missing values replaced by the mode.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()  # Work with a copy to avoid modifying the original DataFrame\n",
    "    if columns is None:\n",
    "        columns = df_copy.columns.tolist()  # If no columns are specified, use all columns\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"most_frequent\")  # Create an imputer that uses the most frequent value\n",
    "    df_copy[columns] = imputer.fit_transform(df_copy[columns])  # Impute missing values in the specified columns\n",
    "    return df_copy  # Return the DataFrame with imputed values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d2ff6",
   "metadata": {},
   "source": [
    "## 1.4 Prepare the folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "497d33f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data directory\n",
    "!rm -rf ../data/archiv/\n",
    "!rm -rf ../data/raw/\n",
    "# !rm -rf ../data/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99715c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Folders Structure\n",
    "!mkdir -p ../data/archiv\n",
    "!mkdir -p ../data/raw/kaggle/reviews\n",
    "!mkdir -p ../data/raw/kaggle/congress\n",
    "!mkdir -p ../data/raw/uci/mental_health_risk\n",
    "!mkdir -p ../data/raw/uci/autistic_spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d39bfa9",
   "metadata": {},
   "source": [
    "## 1.3. Load The Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d9a8e",
   "metadata": {},
   "source": [
    "- From UCI Machine Learning Repository\n",
    "  - Maternal Health Risk -> https://archive.ics.uci.edu/dataset/863/maternal+health+risk\n",
    "  - Autistic Spectrum Disorder Screening Data for Children -> https://archive.ics.uci.edu/dataset/419/autistic+spectrum+disorder+screening+data+for+children\n",
    "- From Kaggle\n",
    "  - 184.702 TU ML 2025S - Reviews -> https://www.kaggle.com/competitions/184-702-tu-ml-2025-s-reviews/data\n",
    "  - 184.702 TU ML 2025S - Congressional Voting -> https://www.kaggle.com/competitions/184-702-tu-ml-2025-s-congressional-voting/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8ec020",
   "metadata": {},
   "source": [
    "### 1.3.1 Download the Kaggle Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d7995e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 19:10:53,954 - __main__ - INFO - Reviews dataset downloaded and extracted.\n",
      "2025-04-27 19:10:55,376 - __main__ - INFO - Congressional Voting Records dataset downloaded and extracted.\n"
     ]
    }
   ],
   "source": [
    "# Reviews-Dataset in ../data/reviews speichern\n",
    "!kaggle competitions download -c 184-702-tu-ml-2025-s-reviews -p ../data/archiv/kaggle/\n",
    "!unzip -qo ../data/archiv/kaggle/184-702-tu-ml-2025-s-reviews.zip -d ../data/raw/kaggle/reviews/\n",
    "logger.info(\"Reviews dataset downloaded and extracted.\")\n",
    "\n",
    "# Congressional Voting Records in ../data/congress speichern\n",
    "!kaggle competitions download -c 184-702-tu-ml-2025-s-congressional-voting -p ../data/archiv/kaggle/\n",
    "!unzip -qo ../data/archiv/kaggle/184-702-tu-ml-2025-s-congressional-voting.zip -d ../data/raw/kaggle/congress/\n",
    "logger.info(\"Congressional Voting Records dataset downloaded and extracted.\")\n",
    "\n",
    "# Load the datasets into a pandas DataFrame\n",
    "df_reviews_raw = pd.read_csv(\"../data/raw/kaggle/reviews/amazon_review_ID.shuf.lrn.csv\")\n",
    "df_congress_raw = pd.read_csv(\"../data/raw/kaggle/congress/CongressionalVotingID.shuf.lrn.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98943b29",
   "metadata": {},
   "source": [
    "### 1.3.2 Download the UCI Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ae37a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 19:11:29,934 - __main__ - INFO - Maternal Health Risk dataset successfully saved at ../data/raw/uci/mental_health_risk/maternal_health_risk.csv\n",
      "2025-04-27 19:11:33,055 - __main__ - INFO - Autistic Spectrum dataset successfully saved at ../data/raw/uci/autistic_spectrum/asd_screening.csv\n"
     ]
    }
   ],
   "source": [
    "# ------- Mental Health Risk ------- #\n",
    "# Download the mental health risk dataset\n",
    "# fetch dataset \n",
    "maternal_health_risk = fetch_ucirepo(id=863) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_maternal = maternal_health_risk.data.features\n",
    "y_maternal = maternal_health_risk.data.targets\n",
    "df_maternal = pd.concat([X_maternal, y_maternal], axis=1)\n",
    "  \n",
    "# Save the dataset to CSV\n",
    "maternal_path = '../data/raw/uci/mental_health_risk/maternal_health_risk.csv'\n",
    "df_maternal.to_csv(maternal_path, index=False)\n",
    "logger.info(f\"Maternal Health Risk dataset successfully saved at {maternal_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------- Autistic Spectrum ------- #\n",
    "# Download the autistic spectrum dataset\n",
    "# fetch dataset \n",
    "asd_data = fetch_ucirepo(id=419) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_asd = asd_data.data.features\n",
    "y_asd = asd_data.data.targets\n",
    "df_asd = pd.concat([X_asd, y_asd], axis=1)\n",
    "  \n",
    "# Save the dataset to CSV\n",
    "asd_path = '../data/raw/uci/autistic_spectrum/asd_screening.csv'\n",
    "df_asd.to_csv(asd_path, index=False)\n",
    "logger.info(f\"Autistic Spectrum dataset successfully saved at {asd_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f64871e",
   "metadata": {},
   "source": [
    "# 2. Data Analysis & Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222f691",
   "metadata": {},
   "source": [
    "# 2.1. Maternal Health Risk Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07949a2b",
   "metadata": {},
   "source": [
    "# 2.2. Autistic Spectrum Disorder Screening Data for Children"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb5bb62",
   "metadata": {},
   "source": [
    "# 2.3. Amazon Review Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec33bc6f",
   "metadata": {},
   "source": [
    "# 2.4. Congressional Voting Data Set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
